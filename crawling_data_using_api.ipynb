{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import json\n",
    "import requests \n",
    "from clickhouse_driver import Client\n",
    "client = Client('localhost')\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_json(method, url, params):\n",
    "    \"\"\"\n",
    "    function for crawling\n",
    "    raise error when http status code is not 200 (success).\n",
    "    \n",
    "    @param method: (str)  http method for request\n",
    "    @param url   : (str)  request url\n",
    "    @param params: (dict) parameter for request\n",
    "    \"\"\"\n",
    "    if method == 'get':\n",
    "        res = requests.get(url, params=params)\n",
    "    elif method == 'post':\n",
    "        res = requests.post(URL, data=json.dumps(data))\n",
    "        \n",
    "    res.raise_for_status()\n",
    "    \n",
    "    return res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API를 활용한 크롤링\n",
    "* 데이터를 가져온 url은 공개하지 않겠음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config file 가져오기\n",
    "with open('config.json') as f:\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 대분류와 중분류 가져오기\n",
    "\n",
    "`Example`\n",
    "```python\n",
    "# sample\n",
    "list_url = json_data['get_crawldata']['get_category_url']\n",
    "list_param = {\n",
    "    'display': 'tree',\n",
    "    'kind': 'category',\n",
    "    'lang': 'ko',\n",
    "    'v': 'v1'\n",
    "}\n",
    "\n",
    "crawl_json('get', list_url, list_param)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['tags'][1]['title']\n",
    "# test['tags'][1]['id']\n",
    "\n",
    "# test['tags'][1]['tags'][0]['title']\n",
    "# test['tags'][1]['tags'][0]['id']\n",
    "# test['tags'][1]['tags'][0]['converted_tag_for_url']\n",
    "# test['tags'][1]['tags'][0]['count_job']\n",
    "# test['tags'][1]['tags'][0]['count_user']\n",
    "\n",
    "list_url = json_data['get_crawldata']['get_category_url']\n",
    "\n",
    "list_param = {\n",
    "    'display': 'tree',\n",
    "    'kind': 'category',\n",
    "    'lang': 'ko',\n",
    "    'v': 'v1'\n",
    "}\n",
    "\n",
    "contents = crawl_json('get', list_url, list_param)\n",
    "\n",
    "\n",
    "main_df = pd.DataFrame()\n",
    "main_lst = []\n",
    "_col_lst = ['main_cate', 'main_cate_id', 'main_cate_en', 'medium_cate', 'medium_cate_id', 'medium_cate_en', 'medium_cate_cnt_job', 'medium_cate_cnt_user']\n",
    "for i in range(len(contents['tags'])):\n",
    "    \n",
    "    _main_lst = [[contents['tags'][i]['title'], contents['tags'][i]['id'], contents['tags'][i]['mapping_legacy'], contents['tags'][i]['tags'][j]['title'], contents['tags'][i]['tags'][j]['id'], contents['tags'][i]['tags'][j]['converted_tag_for_url'], contents['tags'][i]['tags'][j]['count_job'], contents['tags'][i]['tags'][j]['count_user']] if contents['tags'][i]['mapping_legacy']!= None else [contents['tags'][i]['title'], contents['tags'][i]['id'], contents['tags'][i]['title'], contents['tags'][i]['tags'][j]['title'], contents['tags'][i]['tags'][j]['id'], contents['tags'][i]['tags'][j]['converted_tag_for_url'], contents['tags'][i]['tags'][j]['count_job'], contents['tags'][i]['tags'][j]['count_user']] for j in range(len(contents['tags'][i]['tags']))]    \n",
    "#     _main_lst = [test['tags'][i]['title'], test['tags'][i]['id'], test['tags'][i]['mapping_legacy'], test['tags'][i]['tags'][0]['title'], test['tags'][i]['tags'][0]['id'], test['tags'][i]['tags'][0]['converted_tag_for_url'], test['tags'][i]['tags'][0]['count_job'], test['tags'][i]['tags'][0]['count_user']]\n",
    "        \n",
    "    _main_df = pd.DataFrame(_main_lst, columns=_col_lst)\n",
    "\n",
    "    main_df = main_df.append(_main_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  main_cate  main_cate_id main_cate_en medium_cate  medium_cate_id  \\\n0       F&B         10057          F&B       식품 MD             761   \n1       F&B         10057          F&B         요리사             748   \n2       F&B         10057          F&B     외식업 종사자             749   \n3       F&B         10057          F&B         바텐더             747   \n4       F&B         10057          F&B  외식업 메뉴 개발자           10058   \n\n          medium_cate_en  medium_cate_cnt_job  medium_cate_cnt_user  \n0                food md                    8                  2125  \n1                   chef                    6                  4172  \n2      restaurant worker                    4                  5901  \n3              bartender                    2                  1006  \n4  restaurant management                    2                  1867  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>main_cate</th>\n      <th>main_cate_id</th>\n      <th>main_cate_en</th>\n      <th>medium_cate</th>\n      <th>medium_cate_id</th>\n      <th>medium_cate_en</th>\n      <th>medium_cate_cnt_job</th>\n      <th>medium_cate_cnt_user</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>F&amp;B</td>\n      <td>10057</td>\n      <td>F&amp;B</td>\n      <td>식품 MD</td>\n      <td>761</td>\n      <td>food md</td>\n      <td>8</td>\n      <td>2125</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>F&amp;B</td>\n      <td>10057</td>\n      <td>F&amp;B</td>\n      <td>요리사</td>\n      <td>748</td>\n      <td>chef</td>\n      <td>6</td>\n      <td>4172</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>F&amp;B</td>\n      <td>10057</td>\n      <td>F&amp;B</td>\n      <td>외식업 종사자</td>\n      <td>749</td>\n      <td>restaurant worker</td>\n      <td>4</td>\n      <td>5901</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>F&amp;B</td>\n      <td>10057</td>\n      <td>F&amp;B</td>\n      <td>바텐더</td>\n      <td>747</td>\n      <td>bartender</td>\n      <td>2</td>\n      <td>1006</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>F&amp;B</td>\n      <td>10057</td>\n      <td>F&amp;B</td>\n      <td>외식업 메뉴 개발자</td>\n      <td>10058</td>\n      <td>restaurant management</td>\n      <td>2</td>\n      <td>1867</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[]"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "create_tbl = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS crawl.wanted_cate (\n",
    "    `insert_time` DateTime DEFAULT now(),\n",
    "    `main_cate` String, \n",
    "    `main_cate_id` UInt32, \n",
    "    `main_cate_en` String,\n",
    "    `medium_cate` String, \n",
    "    `medium_cate_id` UInt32,\n",
    "    `medium_cate_en` String,\n",
    "    `medium_cate_cnt_job` UInt16,\n",
    "    `medium_cate_cnt_user` UInt32\n",
    "\n",
    ") ENGINE = MergeTree PARTITION BY insert_time ORDER BY main_cate_id SETTINGS index_granularity = 8192\n",
    "\"\"\"\n",
    "\n",
    "client.execute(create_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "344"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "client.execute('INSERT INTO crawl.wanted_cate (main_cate, main_cate_id, main_cate_en, medium_cate, medium_cate_id, medium_cate_en, medium_cate_cnt_job, medium_cate_cnt_user) VALUES', main_df.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 중분류 마다 구인을 하고 있는 회사 리스트 가져오기\n",
    "관심있는 직군에서만 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['F&B' '개발' '건설, 시설' '게임 제작' '경영, 비즈니스' '고객서비스, 리테일' '디자인' '마케팅, 광고'\n '물류, 무역' '미디어' '법률, 법집행기관' '엔지니어링, 설계' '영업' '의료, 제약, 바이오' '인사, 교육'\n '정부, 비영리' '제조, 생산' '금융']\n['식품 MD' '요리사' '외식업 종사자' '바텐더' '외식업 메뉴 개발자' '제과제빵사' '레스토랑 관리자' '영양사'\n '서버 개발자' '웹 개발자']\n"
    }
   ],
   "source": [
    "print(main_df['main_cate'].unique())\n",
    "print(main_df['medium_cate'].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookmark_cate_lst = ['데이터 엔지니어', '빅데이터 엔지니어', '데이터 사이언티스트', '머신러닝 엔지니어', '서비스 기획자', '사업개발,기획자', '프로젝트 매니저', '전략 기획자', '데이터 분석가']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   main_cate  main_cate_id main_cate_en medium_cate  medium_cate_id  \\\n15        개발           518  Development    데이터 엔지니어             655   \n20        개발           518  Development   머신러닝 엔지니어            1634   \n21        개발           518  Development  데이터 사이언티스트            1024   \n23        개발           518  Development   빅데이터 엔지니어            1025   \n65  경영, 비즈니스           507     Business     서비스 기획자             565   \n66  경영, 비즈니스           507     Business    사업개발,기획자             564   \n67  경영, 비즈니스           507     Business    프로젝트 매니저             559   \n68  경영, 비즈니스           507     Business      전략 기획자             563   \n70  경영, 비즈니스           507     Business     데이터 분석가             656   \n\n               medium_cate_en  medium_cate_cnt_job  medium_cate_cnt_user  \n15              data engineer                  196                  9387  \n20  machine learning engineer                  132                  4636  \n21             data scientist                  131                  6189  \n23          big data engineer                  109                  4879  \n65            service planner                  286                 15387  \n66       business development                  259                 18525  \n67            project manager                  234                 15712  \n68          strategic planner                  187                 15799  \n70               data analyst                  113                  7689  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>main_cate</th>\n      <th>main_cate_id</th>\n      <th>main_cate_en</th>\n      <th>medium_cate</th>\n      <th>medium_cate_id</th>\n      <th>medium_cate_en</th>\n      <th>medium_cate_cnt_job</th>\n      <th>medium_cate_cnt_user</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>개발</td>\n      <td>518</td>\n      <td>Development</td>\n      <td>데이터 엔지니어</td>\n      <td>655</td>\n      <td>data engineer</td>\n      <td>196</td>\n      <td>9387</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>개발</td>\n      <td>518</td>\n      <td>Development</td>\n      <td>머신러닝 엔지니어</td>\n      <td>1634</td>\n      <td>machine learning engineer</td>\n      <td>132</td>\n      <td>4636</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>개발</td>\n      <td>518</td>\n      <td>Development</td>\n      <td>데이터 사이언티스트</td>\n      <td>1024</td>\n      <td>data scientist</td>\n      <td>131</td>\n      <td>6189</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>개발</td>\n      <td>518</td>\n      <td>Development</td>\n      <td>빅데이터 엔지니어</td>\n      <td>1025</td>\n      <td>big data engineer</td>\n      <td>109</td>\n      <td>4879</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>경영, 비즈니스</td>\n      <td>507</td>\n      <td>Business</td>\n      <td>서비스 기획자</td>\n      <td>565</td>\n      <td>service planner</td>\n      <td>286</td>\n      <td>15387</td>\n    </tr>\n    <tr>\n      <th>66</th>\n      <td>경영, 비즈니스</td>\n      <td>507</td>\n      <td>Business</td>\n      <td>사업개발,기획자</td>\n      <td>564</td>\n      <td>business development</td>\n      <td>259</td>\n      <td>18525</td>\n    </tr>\n    <tr>\n      <th>67</th>\n      <td>경영, 비즈니스</td>\n      <td>507</td>\n      <td>Business</td>\n      <td>프로젝트 매니저</td>\n      <td>559</td>\n      <td>project manager</td>\n      <td>234</td>\n      <td>15712</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>경영, 비즈니스</td>\n      <td>507</td>\n      <td>Business</td>\n      <td>전략 기획자</td>\n      <td>563</td>\n      <td>strategic planner</td>\n      <td>187</td>\n      <td>15799</td>\n    </tr>\n    <tr>\n      <th>70</th>\n      <td>경영, 비즈니스</td>\n      <td>507</td>\n      <td>Business</td>\n      <td>데이터 분석가</td>\n      <td>656</td>\n      <td>data analyst</td>\n      <td>113</td>\n      <td>7689</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "main_df[main_df['medium_cate'].isin(bookmark_cate_lst)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = main_df[main_df['medium_cate'].isin(bookmark_cate_lst)].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[]"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "create_tbl = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS crawl.wanted_job_id (\n",
    "        `insert_time` DateTime DEFAULT now(),\n",
    "        `company_name` String,\n",
    "        `company_id` UInt32,\n",
    "        `industry_name` String,\n",
    "        `country` String,\n",
    "        `location` String,\n",
    "        `job_id` UInt32,\n",
    "        `job_position` String,\n",
    "        `job_like_count` UInt32,\n",
    "        `main_cate_id` UInt32,\n",
    "        `medium_cate` String,\n",
    "        `medium_cate_id` UInt32\n",
    "\n",
    "    ) ENGINE = MergeTree PARTITION BY main_cate_id ORDER BY main_cate_id SETTINGS index_granularity = 8192\n",
    "\"\"\"\n",
    "\n",
    "client.execute(create_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'INSERT INTO crawl.wanted_job_id (company_name,company_id,industry_name,country,location,job_id,job_position,job_like_count,main_cate_id,medium_cate,medium_cate_id) VALUES'"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "columns = ['company_name','company_id','industry_name','country','location','job_id','job_position','job_like_count','main_cate_id','medium_cate','medium_cate_id']\n",
    "\n",
    "\n",
    "# print('`{}`'.format('`,`'.join(total_df.columns.tolist())).replace(',', ',\\n'))\n",
    "\n",
    "val_columns = ','.join(columns)\n",
    "'INSERT INTO crawl.wanted_job_id ({}) VALUES'.format(val_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "518 데이터 엔지니어 655\noffset_num: 0\noffset_num: 100\n====> There is No more offset_num: 200\nend\n===> To Clickhouse database\n518 머신러닝 엔지니어 1634\noffset_num: 0\noffset_num: 100\n====> There is No more offset_num: 200\nend\n===> To Clickhouse database\n518 데이터 사이언티스트 1024\noffset_num: 0\noffset_num: 100\n====> There is No more offset_num: 200\nend\n===> To Clickhouse database\n518 빅데이터 엔지니어 1025\noffset_num: 0\noffset_num: 100\n====> There is No more offset_num: 200\nend\n===> To Clickhouse database\n507 서비스 기획자 565\noffset_num: 0\noffset_num: 100\noffset_num: 200\n====> There is No more offset_num: 300\nend\n===> To Clickhouse database\n507 사업개발,기획자 564\noffset_num: 0\noffset_num: 100\noffset_num: 200\n====> There is No more offset_num: 300\nend\n===> To Clickhouse database\n507 프로젝트 매니저 559\noffset_num: 0\noffset_num: 100\noffset_num: 200\n====> There is No more offset_num: 300\nend\n===> To Clickhouse database\n507 전략 기획자 563\noffset_num: 0\noffset_num: 100\n====> There is No more offset_num: 200\nend\n===> To Clickhouse database\n507 데이터 분석가 656\noffset_num: 0\n====> There is No more offset_num: 100\nend\n===> To Clickhouse database\n00:00:49\n"
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "##### loop medium_cate #####\n",
    "list_url = json_data['get_crawldata']['get_company_url']\n",
    "total_df = pd.DataFrame()\n",
    "\n",
    "### SET\n",
    "limit_num = 100\n",
    "\n",
    "for i in range(len(df)):\n",
    "    MAIN_CATE_ID = df.iloc[i]['main_cate_id']\n",
    "    MEDIUM_CATE = df.iloc[i]['medium_cate']\n",
    "    MEDIUM_CATE_ID = df.iloc[i]['medium_cate_id']\n",
    "    \n",
    "    print(MAIN_CATE_ID, MEDIUM_CATE, MEDIUM_CATE_ID)\n",
    "    \n",
    "    ### SET \n",
    "    learning_state = True\n",
    "    offset_num = 0\n",
    "    entire_data = []\n",
    "\n",
    "    \n",
    "    while learning_state:\n",
    "\n",
    "        list_param = {\n",
    "        'country': 'kr',\n",
    "        'tag_type_id': str(MEDIUM_CATE_ID),\n",
    "        'job_sort': 'job.latest_order',\n",
    "        'years': -1,\n",
    "        'locations': 'all',\n",
    "        'offset': offset_num,\n",
    "        'limit': limit_num}\n",
    "\n",
    "        list_res = crawl_json('get', list_url, list_param) # data가 있는지 없는지 확인하기 위함\n",
    "\n",
    "        if list_res['data'] == None: # 데이터가 없으면 learning_state = False 로 주어 다음이 안 돌아가도록 함\n",
    "            print(\"====> There is No more offset_num: {}\".format(offset_num))\n",
    "            print('end')\n",
    "            learning_state = False\n",
    "\n",
    "        else: # 데이터가 있는 경우는 계속 저장\n",
    "            print(\"offset_num: {}\".format(offset_num))\n",
    "    #         print(list_res)\n",
    "            entire_data += list_res['data']\n",
    "#             offset_num += 1\n",
    "            offset_num += limit_num\n",
    "            \n",
    "        if (offset_num%120 == 0 and offset_num!=0):\n",
    "            wait_min = 15\n",
    "            print(\"Sleep {}min\".format(15/60))\n",
    "            time.sleep(wait_min/60)\n",
    "    \n",
    "    \n",
    "    # ENTIRE_DATA NEED PARSING\n",
    "    info_col_name = ['company_name', 'company_id', 'industry_name', 'country', 'location', 'job_id', 'job_position', 'job_like_count']\n",
    "    list_info = [[i['company']['name'], i['company']['id'], i['company']['industry_name'], i['address']['country'], i['address']['location'], i['id'], i['position'],  i['like_count']] for i in entire_data]\n",
    "#     print(list_info)\n",
    "#     print(info_col_name)\n",
    "    \n",
    "    \n",
    "    # set main_cate_id, medium_cate, medium_cate_id\n",
    "    _total_df = pd.DataFrame(list_info, columns=info_col_name)\n",
    "    _total_df['main_cate_id'] = MAIN_CATE_ID\n",
    "    _total_df['medium_cate'] = MEDIUM_CATE\n",
    "    _total_df['medium_cate_id'] = MEDIUM_CATE_ID\n",
    "    \n",
    "    # val_columns 은 Column명 정의\n",
    "    print(\"===> To Clickhouse database\")\n",
    "    client.execute('INSERT INTO crawl.wanted_job_id ({}) VALUES'.format(val_columns), _total_df.values.tolist())\n",
    "    \n",
    "#     print(_total_df.head())\n",
    "    \n",
    "    total_df = total_df.append(_total_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Index(['company_name', 'company_id', 'industry_name', 'country', 'location',\n       'job_id', 'job_position', 'job_like_count', 'main_cate_id',\n       'medium_cate', 'medium_cate_id'],\n      dtype='object')\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "      company_id  job_id             job_position\n0           1571   39035        데이터 분석가(비즈니스 분석가)\n1           4411   40389            Data Engineer\n2          13452   40420         공간정보 분야 연구개발 연구원\n3           4857   40430                데이터분석 전문가\n4           4857   40422               DBA 부문 전문가\n...          ...     ...                      ...\n1474         137   30399                VC전략팀 시니어\n1475         886   29182  [인텔리전스랩스] 분석지원팀 데이터 분석가\n1476         331   25065                  서비스 기획자\n1477        2330   23846        Data Privacy Lead\n1478        3091   19416                  플랫폼 기획자\n\n[1479 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>company_id</th>\n      <th>job_id</th>\n      <th>job_position</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1571</td>\n      <td>39035</td>\n      <td>데이터 분석가(비즈니스 분석가)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4411</td>\n      <td>40389</td>\n      <td>Data Engineer</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>13452</td>\n      <td>40420</td>\n      <td>공간정보 분야 연구개발 연구원</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4857</td>\n      <td>40430</td>\n      <td>데이터분석 전문가</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4857</td>\n      <td>40422</td>\n      <td>DBA 부문 전문가</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1474</th>\n      <td>137</td>\n      <td>30399</td>\n      <td>VC전략팀 시니어</td>\n    </tr>\n    <tr>\n      <th>1475</th>\n      <td>886</td>\n      <td>29182</td>\n      <td>[인텔리전스랩스] 분석지원팀 데이터 분석가</td>\n    </tr>\n    <tr>\n      <th>1476</th>\n      <td>331</td>\n      <td>25065</td>\n      <td>서비스 기획자</td>\n    </tr>\n    <tr>\n      <th>1477</th>\n      <td>2330</td>\n      <td>23846</td>\n      <td>Data Privacy Lead</td>\n    </tr>\n    <tr>\n      <th>1478</th>\n      <td>3091</td>\n      <td>19416</td>\n      <td>플랫폼 기획자</td>\n    </tr>\n  </tbody>\n</table>\n<p>1479 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "print(total_df.columns)\n",
    "total_df[['company_id', 'job_id', 'job_position']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 각 상세정보 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "9"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "total_df['medium_cate_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_df = total_df[['company_id', 'company_name', 'job_id', 'job_position', 'medium_cate_id', 'medium_cate', 'location']].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobDesc(i):\n",
    "    \"\"\"\n",
    "    i(int): indes of Dataframe\n",
    "    \"\"\"\n",
    "    detail_url = base_url.format(job_df.iloc[i]['job_id'])\n",
    "    detail_res = crawl_json('get', detail_url, {}) \n",
    "    detail = detail_res['job']['detail']\n",
    "    \n",
    "    for k, v in detail.items():\n",
    "        detail[k] = [re.sub(r'[\\t\\r\\n]', '', s) for s in re.split('[•|-]', detail[k]) if s not in ['']] \n",
    "        \n",
    "    \n",
    "    company_id = job_df.iloc[i]['company_id']\n",
    "    company_name = job_df.iloc[i]['company_name']\n",
    "    job_id = job_df.iloc[i]['job_id']\n",
    "    job_position = job_df.iloc[i]['job_position']\n",
    "    medium_cate_id = job_df.iloc[i]['medium_cate_id']\n",
    "    medium_cate = job_df.iloc[i]['medium_cate']\n",
    "    location = job_df.iloc[i]['location']\n",
    "    \n",
    "    desc_col_list = ['requirements', 'main_tasks', 'preferred_points', 'benefits', 'intro']\n",
    "    \n",
    "    desc_dict = {}\n",
    "    \n",
    "    for ii in desc_col_list:\n",
    "        try:\n",
    "            desc_dict[ii] = detail[ii]\n",
    "        except KeyError as e:\n",
    "            desc_dict[ii] = detail.get(e, list(\"-\"))\n",
    "#             print(\"{} e :: {} {} {}\".format(i, e, detail_url, desc_dict[ii]))\n",
    "            print(\"{} e :: {} {}\".format(i, e, desc_dict[ii]))\n",
    "            \n",
    "    new_data = [company_id, company_name, job_id, job_position, medium_cate_id, medium_cate, location, desc_dict['requirements'], desc_dict['main_tasks'], desc_dict['preferred_points'], desc_dict['benefits'], desc_dict['intro']]\n",
    "    \n",
    "    if (i % 150 == 0 and i != 0):\n",
    "#         print('---->{}'.format(detail_url))\n",
    "        print('---->{}'.format('getting {} detail_url'.format(i)))\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "        time.sleep(10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return pd.DataFrame(new_data).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[]"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# Database에 저장하기 위한 Table을 만든다. \n",
    "\n",
    "create_tbl = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS crawl.wanted_job_desc (\n",
    "        `insert_time` DateTime DEFAULT now(),\n",
    "        `company_id` UInt32,\n",
    "        `company_name` String, \n",
    "        `job_id` UInt32,   \n",
    "        `job_position` String,\n",
    "        `medium_cate_id` UInt32, \n",
    "        `medium_cate` String,  \n",
    "        `location` String, \n",
    "        `requirements` Array(String),  \n",
    "        `main_tasks` Array(String),    \n",
    "        `preferred_points` Array(String),  \n",
    "        `benefits` Array(String),  \n",
    "        `intro` Array(String)\n",
    "    ) ENGINE = MergeTree PARTITION BY insert_time ORDER BY insert_time SETTINGS index_granularity = 8192    \n",
    "\"\"\"\n",
    "\n",
    "client.execute(create_tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "`company_id`,\n`company_name`,\n`job_id`,\n`job_position`,\n`medium_cate_id`,\n`medium_cate`,\n`location`,\n`requirements`,\n`main_tasks`,\n`preferred_points`,\n`benefits`,\n`intro`\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'INSERT INTO crawl.wanted_job_desc (company_id,company_name,job_id,job_position,medium_cate_id,medium_cate,location,requirements,main_tasks,preferred_points,benefits,intro) VALUES'"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "columns = ['company_id',\n",
    " 'company_name',\n",
    " 'job_id',\n",
    " 'job_position',\n",
    " 'medium_cate_id',\n",
    " 'medium_cate',\n",
    " 'location',\n",
    " 'requirements',\n",
    " 'main_tasks',\n",
    " 'preferred_points',\n",
    " 'benefits',\n",
    " 'intro']\n",
    "\n",
    "\n",
    "print('`{}`'.format('`,`'.join(columns)).replace(',', ',\\n'))\n",
    "\n",
    "val_columns = ','.join(columns)\n",
    "'INSERT INTO crawl.wanted_job_desc ({}) VALUES'.format(val_columns)\n",
    "\n",
    "# client.execute('INSERT INTO crawl.wanted_job_desc ({}) VALUES'.format(val_columns), _total_df.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 100\n0/15\n===> To Clickhouse database\n00:00:47\n100 200\n---->getting 150 detail_url\n00:01:10\n1/15\n===> To Clickhouse database\n00:01:39\n200 300\n2/15\n===> To Clickhouse database\n00:02:19\n300 400\n---->getting 300 detail_url\n00:02:19\n3/15\n===> To Clickhouse database\n00:03:03\n400 500\n---->getting 450 detail_url\n00:03:19\n4/15\n===> To Clickhouse database\n00:03:45\n500 600\n5/15\n===> To Clickhouse database\n00:04:23\n600 700\n---->getting 600 detail_url\n00:04:23\n6/15\n===> To Clickhouse database\n00:05:09\n700 800\n700 e :: 'preferred_points' ['-']\n---->getting 750 detail_url\n00:05:31\n7/15\n===> To Clickhouse database\n00:06:00\n800 900\n8/15\n===> To Clickhouse database\n00:06:33\n900 1000\n---->getting 900 detail_url\n00:06:33\n991 e :: 'preferred_points' ['-']\n9/15\n===> To Clickhouse database\n00:07:18\n1000 1100\n1001 e :: 'preferred_points' ['-']\n---->getting 1050 detail_url\n00:07:41\n10/15\n===> To Clickhouse database\n00:08:10\n1100 1200\n1156 e :: 'preferred_points' ['-']\n11/15\n===> To Clickhouse database\n00:08:44\n1200 1300\n---->getting 1200 detail_url\n00:08:44\n1214 e :: 'preferred_points' ['-']\n12/15\n===> To Clickhouse database\n00:09:31\n1300 1400\n---->getting 1350 detail_url\n00:09:49\n1383 e :: 'preferred_points' ['-']\n13/15\n===> To Clickhouse database\n00:10:18\n1400 1479\n1444 e :: 'preferred_points' ['-']\n14/15\n===> To Clickhouse database\n00:10:45\n00:10:45\n"
    }
   ],
   "source": [
    "# get job detail\n",
    "# 조금씩 데이터를 설정해서 넣는 방법?? => Batch\n",
    "\n",
    "base_url = json_data['get_crawldata']['get_desc_url']\n",
    "\n",
    "import re\n",
    "import copy \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "### SET ###\n",
    "# dataframe for batch Insert\n",
    "# row_num [0:100], [101:200], ....\n",
    "# While False => insert \n",
    "\n",
    "\n",
    "batch_size = 100 # 몇개씩 insert 할 것인가\n",
    "total_step = len(job_df)//batch_size\n",
    "\n",
    "_total_df = pd.DataFrame()\n",
    "\n",
    "for step in range(total_step+1):\n",
    "    new_df = pd.DataFrame()\n",
    "    \n",
    "    if (step+1)*batch_size < len(job_df):\n",
    "        print(step*batch_size, (step+1)*batch_size)\n",
    "        \n",
    "        for i in range(step*batch_size, (step+1)*batch_size):\n",
    "            _job_df = get_jobDesc(i)\n",
    "            new_df = new_df.append(_job_df, ignore_index=False)\n",
    "            \n",
    "            \n",
    "    else:\n",
    "#         print((step)*batch_size, len(job_df)-(step)*batch_size+(step*batch_size))\n",
    "        print(step*batch_size, len(job_df))\n",
    "        for i in range(step*batch_size, len(job_df)):\n",
    "            _job_df = get_jobDesc(i)\n",
    "            new_df = new_df.append(_job_df, ignore_index=False)\n",
    "    \n",
    "    print('{}/{}'.format(step, total_step+1))\n",
    "#     new_df = new_df.append(new_df, ignore_index=True)\n",
    "    \n",
    "    print(\"===> To Clickhouse database\")\n",
    "    client.execute('INSERT INTO crawl.wanted_job_desc ({}) VALUES'.format(val_columns), new_df.values.tolist())                                  \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n",
    "    _total_df = _total_df.append(new_df, ignore_index=True)\n",
    "elapsed_time = time.time() - start_time\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593944775186",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}